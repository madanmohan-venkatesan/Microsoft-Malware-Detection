
## Import necessary libraries
import os
import numpy as np
import pandas as pd
from tqdm import tqdm
from scipy.sparse import csr_matrix,vstack,save_npz,load_npz
from time import time
from joblib import Parallel,delayed
from train_test_cv_split import train_test_cv_split
## Train-test-cv split
X_train,X_test,X_cv,y_train,y_test,y_cv=train_test_cv_split()

## Constants
def get_path():
    NOTEBOOK_PATH=os.getcwd()
    PROJECT_ROOT=os.path.dirname(NOTEBOOK_PATH)
    return PROJECT_ROOT

PROJECT_ROOT=get_path()
BYTE_FILES_PATH=os.path.join(PROJECT_ROOT,"data","interim","byteFiles")
OUTPUT_PATH_TRAIN=os.path.join(PROJECT_ROOT,"data","processed","train")
OUTPUT_PATH_TEST=os.path.join(PROJECT_ROOT,"data","processed","test")
OUTPUT_PATH_CV=os.path.join(PROJECT_ROOT,"data","processed","cv")

## Feature extraction from byte files 
def files_splitter(files):
    '''
    Processing list of files for parallel processing. 
    Configured for machine with 10 vCPU. 
    Can be hard coded to attain high throughput
    ``````````````````````````````````````````````````````````````````````````````````
    Args: files: list of filepaths
    ```````````````````````
    Returns: Filepaths splitted equally for each process
    ````````````````````````````````````````````````````
    '''
    n_cpu=10 #Modify if needed
    files_len=len(files)
    partition_files=int(files_len/n_cpu) #hard code if needed

    files_splitted=[files[x:x+partition_files] for x in range(0,files_len,partition_files)] 
    return files_splitted

def vocab_builder(typ):
    """
    Builds vocabulary for unigram and bigrams for byte chars
    ````````````````````````````````````````````````````````
    Args: unigram,bigram
    N-gram type.
    `````````````````````````````````
    Returns: vocab: Unigram/Bigram
    ```````````````````````
    """
    hex_char=[hex(i).replace('0x','') for i in range(256)]
    for i in range(len(hex_char)):
        if len(hex_char[i])==1:
            hex_char[i]='0'+hex_char[i]
            
    if typ=='bigram':
        bi_vocab=[]
        for first in hex_char:
            for second in hex_char:
                bi_vocab.append(first+' '+second)
        return bi_vocab  
    
    return hex_char
from bytes_bigram_extraction import bytes_bigram_extraction
def get_bigram_feature_V2(files,path_to_file):
    path_to_file=os.path.join(path_to_file,"bigram_features_sparse.npz")
    if not os.path.isfile(path_to_file):
        start=time()
        print("Extracting bigram features")
        files_splitted=files_splitter(files)
        vocab_bi=vocab_builder('bigram')
        list_of_bigrams=Parallel(n_jobs=-1)\
            (delayed (bytes_bigram_extraction)(files_splitted_,vocab_bi) for files_splitted_ in files_splitted)
        bigrams=vstack(list_of_bigrams)
        save_npz(path_to_file,bigrams,compressed=True)
        end=time()
        print(f"Extracting bigram features completed. Elapsed time:{round((end-start)/3600,2)} hours")
        
    else:
        print(f"{path_to_file} is already present!")
        bigrams=load_npz(path_to_file)
    return bigrams


filenames_train=[f"{file_name}" for file_name in X_train.Id.values]
filenames_test=[f"{file_name}" for file_name in X_test.Id.values]
filenames_cv=[f"{file_name}" for file_name in X_cv.Id.values]

file_ID_train=[os.path.join(BYTE_FILES_PATH,file_name) for file_name in filenames_train]
file_ID_test=[os.path.join(BYTE_FILES_PATH,file_name) for file_name in filenames_test]
file_ID_cv=[os.path.join(BYTE_FILES_PATH,file_name) for file_name in filenames_cv]

uni_gram_features=vocab_builder('unigram')

print("-"*50)
print("X_train")
unigrams_train=get_bigram_feature_V2(file_ID_train,OUTPUT_PATH_TRAIN)
print("-"*50)


print("X_test")
unigrams_test=get_bigram_feature_V2(file_ID_test,OUTPUT_PATH_TEST)
print("-"*50)

print("X_cv")
unigrams_cv=get_bigram_feature_V2(file_ID_cv,OUTPUT_PATH_CV)
print("-"*50)

