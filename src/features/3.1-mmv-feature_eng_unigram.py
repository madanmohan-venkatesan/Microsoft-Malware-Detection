# Feature engineering for unigram of byte files
## Import necessary libraries
import os
import numpy as np
import pandas as pd
from tqdm import tqdm
from scipy.sparse import csr_matrix,vstack,save_npz,load_npz
from time import time
from joblib import Parallel,delayed
from train_test_cv_split import train_test_cv_split
## Train-test-cv split
X_train,X_test,X_cv,y_train,y_test,y_cv=train_test_cv_split()
## Constants
def get_path():
    NOTEBOOK_PATH=os.getcwd()
    PROJECT_ROOT=os.path.dirname(NOTEBOOK_PATH)
    return PROJECT_ROOT

PROJECT_ROOT=get_path()
BYTE_FILES_PATH=os.path.join(PROJECT_ROOT,"data","interim","byteFiles")
OUTPUT_PATH_TRAIN=os.path.join(PROJECT_ROOT,"data","processed","train")
OUTPUT_PATH_TEST=os.path.join(PROJECT_ROOT,"data","processed","test")
OUTPUT_PATH_CV=os.path.join(PROJECT_ROOT,"data","processed","cv")


## Feature extraction from byte files 
def files_splitter(files):
    '''
    Processing list of files for parallel processing. 
    Configured for machine with 10 vCPU. 
    Can be hard coded to attain high throughput
    ``````````````````````````````````````````````````````````````````````````````````
    Args: files: list of filepaths
    ```````````````````````
    Returns: Filepaths splitted equally for each process
    ````````````````````````````````````````````````````
    '''
    n_cpu=10 #Modify if needed
    files_len=len(files)
    partition_files=int(files_len/n_cpu) #hard code if needed

    files_splitted=[files[x:x+partition_files] for x in range(0,files_len,partition_files)] 
    return files_splitted

#removal of addres from byte files
# ----------------
# contents of .byte files
#00401000 56 8D 44 24 08 50 8B F1 E8 1C 1B 00 00 C7 06 08 
#-------------------
#we remove the starting address 00401000

files = os.listdir(BYTE_FILES_PATH)
filenames=[]
array=[]
for file in tqdm(files):
    if(file.endswith("bytes")):
        file=file.split('.')[0]
        text_file = open(os.path.join(BYTE_FILES_PATH,f"{file}.txt"), 'w+')
        with open(os.path.join(BYTE_FILES_PATH,f"{file}.bytes"),"r") as fp:
            lines=""
            for line in fp:
                a=line.rstrip().split(" ")[1:]
                b=' '.join(a)
                b=b+"\n"
                text_file.write(b)
            fp.close()
            os.remove(os.path.join(BYTE_FILES_PATH,f"{file}.bytes"))
        text_file.close()


def vocab_builder(typ):
    """
    Builds vocabulary for unigram and bigrams for byte chars
    ````````````````````````````````````````````````````````
    Args: unigram,bigram
    N-gram type.
    `````````````````````````````````
    Returns: vocab: Unigram/Bigram
    ```````````````````````
    """
    hex_char=[hex(i).replace('0x','') for i in range(256)]
    for i in range(len(hex_char)):
        if len(hex_char[i])==1:
            hex_char[i]='0'+hex_char[i]
            
    if typ=='bigram':
        bi_vocab=[]
        for first in hex_char:
            for second in hex_char:
                bi_vocab.append(first+' '+second)
        return bi_vocab  
    
    return hex_char

from bytes_unigram_extraction import bytes_unigram_extraction
def get_unigram_feature_v2(files,path_to_file):
    path_to_file=os.path.join(path_to_file,"unigram_features_sparse.npz")
    if not os.path.isfile(path_to_file):
        start=time()
        print("Extracting unigram features")
        files_splitted=files_splitter(files)
        vocab_uni=vocab_builder('unigram')
        
        list_of_unigrams=Parallel(n_jobs=-1)\
            (delayed (bytes_unigram_extraction)(files_splitted_,vocab_uni) for files_splitted_ in files_splitted)
        unigrams=vstack(list_of_unigrams)
        save_npz(path_to_file,unigrams,compressed=True)
        end=time()
        print(f"Extracting unigram features completed. Elapsed time:{round((end-start)/3600,2)} hours")

        
        
    else:
        print(f"{path_to_file} is already present!")
        unigrams=load_npz(path_to_file)
    return unigrams


filenames_train=[f"{file_name}" for file_name in X_train.Id.values]
filenames_test=[f"{file_name}" for file_name in X_test.Id.values]
filenames_cv=[f"{file_name}" for file_name in X_cv.Id.values]

file_ID_train=[os.path.join(BYTE_FILES_PATH,file_name) for file_name in filenames_train]
file_ID_test=[os.path.join(BYTE_FILES_PATH,file_name) for file_name in filenames_test]
file_ID_cv=[os.path.join(BYTE_FILES_PATH,file_name) for file_name in filenames_cv]

uni_gram_features=vocab_builder('unigram')

print("-"*50)
print("X_train")
unigrams_train=get_unigram_feature_v2(file_ID_train,OUTPUT_PATH_TRAIN)
print("-"*50)


print("X_test")
unigrams_test=get_unigram_feature_v2(file_ID_test,OUTPUT_PATH_TEST)
print("-"*50)

print("X_cv")
unigrams_cv=get_unigram_feature_v2(file_ID_cv,OUTPUT_PATH_CV)
print("-"*50)



